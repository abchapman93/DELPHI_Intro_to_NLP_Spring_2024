{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NOTE: This notebook was adapted from an existing notebook from HuggingFace (original here: https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb) by Kelly Peterson for the 2024 DELPHI NLP course at the University of Utah.  It has been modified so that in the first half, a baseline model is trained for classification.  In the second half, we will have a friendly competition to see who can train the best model with the time allowed."
      ],
      "metadata": {
        "id": "13aYMaQK6Xuu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before we get started with code, a quick poll:\n",
        "\n",
        "\n",
        "\n",
        "*   Who here has been skiiing or snowboarding?\n",
        "*   Who has done either of these while wearing a blindfold from an unknown starting location?\n",
        "\n",
        "\n",
        "Source for images : https://www.fromthegenesis.com/gradient-descent-part-2/\n",
        "\n",
        "![hill](https://www.fromthegenesis.com/wp-content/uploads/2018/06/Gradie_Desce.jpg) ![local](https://fromthegenesis.com/wp-content/uploads/2018/06/GDS_5.png)"
      ],
      "metadata": {
        "id": "bQLVTCjaLW-F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install ðŸ¤— Transformers and ðŸ¤— Datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "! pip install datasets transformers transformers[torch] accelerate evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsqqyUAZG0mp"
      },
      "source": [
        "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_zVlHDLG0mp"
      },
      "source": [
        "Then you need to install Git-LFS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SGnevldG0mp"
      },
      "outputs": [],
      "source": [
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's also get our dataset we used for the rules-based part of the class...\n",
        "!pip install https://github.com/abchapman93/DELPHI_Intro_to_NLP_Spring_2024/releases/download/v0.1/delphi_nlp_2024-0.1.tar.gz"
      ],
      "metadata": {
        "id": "HViz_AjoFwri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BThWFw88G0mp"
      },
      "source": [
        "Make sure your version of Transformers is at least 4.11.0 so this notebook can run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbUHBSDVG0mq"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import evaluate\n",
        "\n",
        "import transformers\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "import torch\n",
        "\n",
        "print(transformers.__version__)\n",
        "print(sklearn.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from delphi_nlp_2024 import *\n",
        "from delphi_nlp_2024.quizzes.quizzes import *\n",
        "from delphi_nlp_2024.helpers import *"
      ],
      "metadata": {
        "id": "3LWd5nnCGCcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Fine-tuning a model on a text classification task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RRkXuteIrIh"
      },
      "source": [
        "Some initial parameters for starting our model, even if it's not ideal for our\n",
        "task of PNEUMONIA classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = load_pneumonia_data()\n",
        "\n",
        "test = load_pneumonia_data(\"test\",)\n",
        "\n",
        "print(f'len(train): {len(train)}')\n",
        "print(f'len(test): {len(test)}')"
      ],
      "metadata": {
        "id": "BcIzyCxRVapD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.head())"
      ],
      "metadata": {
        "id": "2Ja300JpGMRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now getting our dataset ready for the HuggingFace libraries.  We start with a dataframe, but we need to convert into dictionaries and then the Dataset type"
      ],
      "metadata": {
        "id": "XubteuSNGi6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now that we have a dataframe, here's a way to iterate through the rows\n",
        "\n",
        "train_dataset_dicts = []\n",
        "test_dataset_dicts = []\n",
        "\n",
        "for index, row in train.iterrows():\n",
        "  text = row['text']\n",
        "  label = row['document_classification']\n",
        "\n",
        "  # key values of text and label\n",
        "  row_dict = {'text': text, 'label': label}\n",
        "  train_dataset_dicts.append(row_dict)\n",
        "\n",
        "for index, row in test.iterrows():\n",
        "  text = row['text']\n",
        "  label = row['document_classification']\n",
        "\n",
        "  # key values of text and label\n",
        "  row_dict = {'text': text, 'label': label}\n",
        "  test_dataset_dicts.append(row_dict)\n",
        "\n",
        "print(f'len(train_dataset_dicts): {len(train_dataset_dicts)}')\n",
        "print(f'len(test_dataset_dicts): {len(train_dataset_dicts)}')"
      ],
      "metadata": {
        "id": "gd2w0NieWluY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now that we have all of the data, let's turn this into a type (Dataset) which HuggingFace recognizes\n",
        "\n",
        "train_dataset = Dataset.from_list(train_dataset_dicts, split=\"train\")\n",
        "test_dataset = Dataset.from_list(test_dataset_dicts, split=\"test\")"
      ],
      "metadata": {
        "id": "Xe6vp6P1YKvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUmphG3IrI3"
      },
      "source": [
        "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3j8APAoIrI3"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=5):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, datasets.ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZy5tRB_IrI7"
      },
      "outputs": [],
      "source": [
        "show_random_elements(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "Before we can feed those texts to our model, we need to preprocess them. This is done by a HuggingFace Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
        "\n",
        "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
        "\n",
        "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
        "- we download the vocabulary used when pretraining this specific checkpoint.\n",
        "\n",
        "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl6IidfdIrJK"
      },
      "source": [
        "We pass along `use_fast=True` to the call above to use one of the fast tokenizers (backed by Rust) from the HuggingFace Tokenizers library. Those fast tokenizers are available for almost all models, but if you got an error with the previous call, remove that argument."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rowT4iCLIrJK"
      },
      "source": [
        "You can directly call this tokenizer on one sentence or a pair of sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5hBlsrHIrJL"
      },
      "outputs": [],
      "source": [
        "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Now that our data is ready, we can download a pretrained base model and fine-tune it. Since all our task is document classification, we use the `AutoModelForSequenceClassification` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us. The only thing we have to specify is the number of labels for our problem (which for PNEUMONIA will be 2.  Either there is a mention of a pneumonia finding or not):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlqNaB8jIrJW"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "num_labels = 2\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CczA5lJlIrJX"
      },
      "source": [
        "The warning is telling us we are throwing away some weights (the `vocab_transform` and `vocab_layer_norm` layers) and randomly initializing some other (the `pre_classifier` and `classifier` layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8urzhyIrJY"
      },
      "source": [
        "To instantiate a `Trainer`, we will need to define two more things. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"accuracy\")"
      ],
      "metadata": {
        "id": "KqWc6nnEni9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bliy8zgjIrJY"
      },
      "outputs": [],
      "source": [
        "metric_name = \"accuracy\"\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-pneumonia\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km3pGVdTIrJc"
      },
      "source": [
        "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay. Since the best model might not be the one at the end of training, we ask the `Trainer` to load the best model it saved (according to `metric_name`) at the end of training."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we continue, we need to tokenize (translate into input_ids for training a model)"
      ],
      "metadata": {
        "id": "o8bFJUqYjMyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens = 512"
      ],
      "metadata": {
        "id": "LiE2DktVl4Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=max_tokens, add_special_tokens = True)\n"
      ],
      "metadata": {
        "id": "dt74uPnhhtRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokenized_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
        "test_tokenized_dataset = test_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
      ],
      "metadata": {
        "id": "QHcJqLTmjb-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "\n",
        "   # Calculate precision, recall, and F1-score\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ],
      "metadata": {
        "id": "jSGPxzUHmu_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXuFTAzDIrJe"
      },
      "source": [
        "Then we just need to pass all of this along with our datasets to the `Trainer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_tokenized_dataset,\n",
        "    eval_dataset=test_tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics = compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibWGmvxbIrJg"
      },
      "source": [
        "You might wonder why we pass along the `tokenizer` when we already preprocessed our data. This is because we will use it once last time to make all the samples we gather the same length by applying padding, which requires knowing the model's preferences regarding padding (to the left or right? with which token?). The `tokenizer` has a pad method that will do all of this right for us, and the `Trainer` will use it. You can customize this part by defining and passing your own `data_collator` which will receive the samples like the dictionaries seen above and will need to return a dictionary of tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "We can now finetune our model by just calling the `train` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNx5pyRlIrJh"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKASz-2vIrJi"
      },
      "source": [
        "Now we can evaluate model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOUcBkX8IrJi"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://rlv.zcache.com/stop_arret_traffic_sign_canada_postcard-rb0ca4e6264f34551ad1219ee2dd55da7_ucbjp_307.jpg\" alt=\"stop\" style=\"width: 100px;height: 100px;\"/>"
      ],
      "metadata": {
        "id": "0NkQKPDS_9Qk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OK, let's consider that to be a baseline model, in this next section, we'll work in small groups or individuals.  The goal is for each participant to explore various hyperparameters that you might want to change.  \n",
        "\n",
        "## We'll do this manually today (which takes time) but it will be a good way to learn and think about these parameters.  In practice, we don't just select parameters randomly like this and wait to see what happens.  Rather, you will likely use a package or strategy for automated hyperparameter search.  For example, these hyperparameters could be:\n",
        "1. Different base model (not the one used above)\n",
        "2. Number of epochs (iterations through the dataset)\n",
        "3. Learning rate\n",
        "4. Weight decay\n",
        "5. Plus many more..."
      ],
      "metadata": {
        "id": "kIiKWERl4LpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO -- set up your own new TrainingArguments here\n",
        "\n",
        "# You can learn more about these parameters, and you can add others which are all documented here:\n",
        "# https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments\n",
        "\n",
        "# We already trained a model up above which will use a lot of RAM (on GPU or CPU)\n",
        "# So let's tell CUDA (the GPU library) to clear its cache...\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# TODO -- change this to something besides None using values below or choose your own...\n",
        "learning_rate_part_2 = None\n",
        "#learning_rate_part_2 = 2e-5\n",
        "#learning_rate_part_2 = 2e-4\n",
        "#learning_rate_part_2 = 2e-3\n",
        "#learning_rate_part_2 = 2e-2\n",
        "\n",
        "# TODO -- change this to something besides None using values below or choose your own...\n",
        "batch_size_part_2 = None\n",
        "#batch_size_part_2 = 2\n",
        "#batch_size_part_2 = 4\n",
        "#batch_size_part_2 = 8\n",
        "#batch_size_part_2 = 16\n",
        "#batch_size_part_2 = 32\n",
        "\n",
        "# TODO -- change this to something besides None using values below or choose your own...\n",
        "num_train_epochs_part_2 = None\n",
        "#num_train_epochs_part_2 = 1\n",
        "#num_train_epochs_part_2 = 2\n",
        "#num_train_epochs_part_2 = 3\n",
        "#num_train_epochs_part_2 = 5\n",
        "#num_train_epochs_part_2 = 10\n",
        "#num_train_epochs_part_2 = 15\n",
        "#num_train_epochs_part_2 = 20\n",
        "#num_train_epochs_part_2 = 25\n",
        "\n",
        "# TODO -- change this to something besides None using values below or choose your own...\n",
        "weight_decay_part_2 = None\n",
        "#weight_decay_part_2 = 0.5\n",
        "#weight_decay_part_2 = 0.1\n",
        "#weight_decay_part_2 = 0.01\n",
        "#weight_decay_part_2 = 0.001\n",
        "#weight_decay_part_2 = 0.0001\n",
        "\n",
        "args_part_2 = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-pneumonia-part-2\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=learning_rate_part_2,\n",
        "    per_device_train_batch_size=batch_size_part_2,\n",
        "    per_device_eval_batch_size=batch_size_part_2,\n",
        "    num_train_epochs=num_train_epochs_part_2,\n",
        "    weight_decay=weight_decay_part_2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name\n",
        ")\n",
        "\n",
        "# you might want to print out what we chose, for keeping track\n",
        "#print(args_part_2)"
      ],
      "metadata": {
        "id": "_e-GS6WP4s_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some of you might want to experiment with different base models (maybe something trained on clinical documents, or medical research).  If you do, make sure that you use a tokenizer which is compatible with your model.  Then if you change this, make sure to retokenize again below"
      ],
      "metadata": {
        "id": "K4TSt7sg7YMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function_part_2(examples):\n",
        "    return tokenizer_part_2(examples[\"text\"], padding=True, truncation=True, max_length=max_tokens_part_2, add_special_tokens = True)"
      ],
      "metadata": {
        "id": "FEYiQiGZ63Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here you select what you want for your base model.  There are many more\n",
        "# models for you to evaluate here\n",
        "# https://huggingface.co/models?pipeline_tag=fill-mask&sort=created\n",
        "# Note in the link above to select the filter under \"Natural Language Processing\"\n",
        "# for \"Fill Mask\" so these are base models whose only task\n",
        "# is filling in the blanks\n",
        "model_name_part_2 = None\n",
        "\n",
        "# You could re-use the model we used above...\n",
        "# https://huggingface.co/distilbert/distilbert-base-uncased\n",
        "#model_name_part_2 = \"distilbert-base-uncased\"\n",
        "\n",
        "# https://huggingface.co/bert-base-uncased\n",
        "#model_name_part_2 = \"bert-base-uncased\"\n",
        "\n",
        "# This one was trained on biomedical literature and clinical data\n",
        "# https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT\n",
        "#model_name_part_2 = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "\n",
        "# Here's a model trained by Microsoft on biomedical abstracts:\n",
        "# https://huggingface.co/microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\n",
        "#model_name_part_2 = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"\n",
        "\n",
        "if model_name_part_2 is None:\n",
        "  print('Careful!  You did not set your base model name so we cannot load anything.')\n",
        "  print('You should see some examples of other possible values you could uncomment...')\n",
        "  raise Exception(\"Check the messages above to set all hyperparameters\")\n",
        "\n",
        "model_part_2 = AutoModelForSequenceClassification.from_pretrained(model_name_part_2, num_labels=num_labels)\n",
        "\n",
        "tokenizer_part_2 = AutoTokenizer.from_pretrained(model_name_part_2, use_fast=True)\n",
        "\n",
        "# if you use a tokenizer here which is different from above, note that you must tokenize again\n",
        "# Each model must align with its tokenizer.  Otherwise, the matrices are all off\n",
        "# (e.g., different widths for token vectors, final vectors, etc...)\n",
        "\n",
        "# make sure you know what the maximum number of tokens can be for your model/tokenizer:\n",
        "max_tokens_part_2 = None\n",
        "#max_tokens_part_2 = 512\n",
        "\n",
        "# make sure to tokenize using the tokenizer you've selected for part 2 here\n",
        "train_tokenized_dataset_part_2 = train_dataset.map(tokenize_function_part_2, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
        "test_tokenized_dataset_part_2 = test_dataset.map(tokenize_function_part_2, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
        "\n",
        "\n",
        "# Attention... let's make sure you set all of your parameters...\n",
        "if learning_rate_part_2 is None or batch_size_part_2 is None or num_train_epochs_part_2 is None or weight_decay_part_2 is None:\n",
        "  print('Careful!  You did not set one of these critical training arugmnets, so you will not be able to train your model.')\n",
        "  print('Go to one of the cells above and make sure you set these to something besides None.')\n",
        "  print('You should see some examples of other possible values you could uncomment...')\n",
        "  raise Exception(\"Check the messages above to set all hyperparameters\")\n",
        "elif model_name_part_2 is None or max_tokens_part_2 is None:\n",
        "  print('Careful!  You did not set your base model name or the max number of tokens.')\n",
        "  print('Go to one of the cells above and make sure you set these to something besides None.')\n",
        "  print('You should see some examples of other possible values you could uncomment...')\n",
        "  raise Exception(\"Check the messages above to set all hyperparameters\")\n",
        "else:\n",
        "  # More documentation on the Trainer can be found here:\n",
        "  # https://huggingface.co/docs/transformers/main_classes/trainer\n",
        "  trainer_part_2 = Trainer(\n",
        "      model_part_2,\n",
        "      args_part_2,\n",
        "      train_dataset=train_tokenized_dataset_part_2,\n",
        "      eval_dataset=test_tokenized_dataset_part_2,\n",
        "      tokenizer=tokenizer_part_2,\n",
        "      compute_metrics = compute_metrics\n",
        "  )\n",
        "\n",
        "  # you might want to print out what we chose for logging our experiments\n",
        "  #print(trainer_part_2)"
      ],
      "metadata": {
        "id": "DLW8GFMQ6ajt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if learning_rate_part_2 is None or batch_size_part_2 is None or num_train_epochs_part_2 is None or weight_decay_part_2 is None:\n",
        "  print('Careful!  You did not set one of these critical training arugmnets, so you will not be able to train your model.')\n",
        "  print('Go to one of the cells above and make sure you set these to something besides None.')\n",
        "  print('You should see some examples of other possible values you could uncomment...')\n",
        "  raise Exception(\"Check the messages above to set all hyperparameters\")\n",
        "elif model_name_part_2 is None or max_tokens_part_2 is None:\n",
        "  print('Careful!  You did not set your base model name or the max number of tokens.')\n",
        "  print('Go to one of the cells above and make sure you set these to something besides None.')\n",
        "  print('You should see some examples of other possible values you could uncomment...')\n",
        "  raise Exception(\"Check the messages above to set all hyperparameters\")\n",
        "else:\n",
        "\n",
        "  print('It looks like you set all your parameters.  Now your model will be trained and evaluated...')\n",
        "  print('This will take some time...')\n",
        "\n",
        "  # now train it!  good luck!!\n",
        "  trainer_part_2.train()\n",
        "\n",
        "  trainer_part_2.evaluate()\n",
        "\n",
        "  print('Done training and evaluating model.')"
      ],
      "metadata": {
        "id": "Plo9iNyH9t2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IOT1wAL69n-l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}