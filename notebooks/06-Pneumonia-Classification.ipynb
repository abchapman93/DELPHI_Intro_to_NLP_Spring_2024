{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://github.com/abchapman93/DELPHI_Intro_to_NLP_Spring_2024/blob/main/media/DELPHI-long.png?raw=true\" size=\"20%\">\n",
    "</br>\n",
    "\n",
    "<h1 valign=\"center\" align=\"center\"><font size=\"+150\">Introduction to NLP in Python</br>Spring 2024</font></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delphi_nlp_2024 import *\n",
    "from delphi_nlp_2024.quizzes.quizzes import *\n",
    "from delphi_nlp_2024.helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import medspacy\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.visualization import visualize_dep, visualize_ent, MedspaCyVisualizerWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last few notebooks we learned the tools for building an NLP system. In this notebook, we'll put all those tools together and build a full classification system using real-world data.\n",
    "\n",
    "## The task\n",
    "We want to identify patients who have radiographic evidence of pneumonia. \n",
    "\n",
    "This task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load the data\n",
    "The dataset in these examples is a set of MIMIC-II radiology reports. The annotations were created by University of Utah physician-scientist and pneumonia extraordinaire [Dr. Barbara Jones](https://healthcare.utah.edu/fad/mddetail.php?physicianID=u0102859&name=barbara-e-jones). As baseline to compare our system against we will use a system recently developed by her team for identifying misdiagnosis of pneumonia in clinical notes: [`medspacy_pna`](https://github.com/abchapman93/medspacy_pneumonia). This was system was designed for VA and University of Utah data, so it might not achieve as high of performance on MIMIC data as what is reported in the paper. Let's see if we can beat its performance!\n",
    "\n",
    "The data is split into two sets: the **training set** and **testing set**. We'll start by developing our system with the training set before doing a final evaluation on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Read in the data\n",
    "Run the code below to read in the training set. The resulting dataframe will have a column for:\n",
    "- The document name\n",
    "- The text\n",
    "- The annotator's document classification (this is the **\"truth\"**), where 1 indicates there is *positive* evidence of pneumonia and 0 indicates there is *not* positive evidence of pneumonia\n",
    "- The baseline NLP system's document classification (this is the **\"prediction\"**)\n",
    "\n",
    "We'll eventually add another column with our own predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_pneumonia_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "The cell below will display the note and annotations for the note with index `idx`. Scroll through the notes and look at the annotations; what phrases seem to be indicative of a positive document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "\n",
    "visualize_pneumonia_annotations(train.iloc[idx][\"html\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document annotation\n",
    "Before building an NLP system we need to define our concepts annotate a corpus of notes to use as a reference standard. We already have an annotated corpus, so we'll review a few short examples and see how we would annotate them and then look at the reference standard annotations that we already have.\n",
    "\n",
    "### 1.1\n",
    "For this task, we will define a **\"POS\"** note as: \n",
    "\n",
    "*A note which contains a positive **or** possible mention of a term referring to pneumonia.*\n",
    "\n",
    "Consider the following terms to be pneumonia:\n",
    "\n",
    "- Pneumonia\n",
    "- Pna\n",
    "- Opacity\n",
    "- Infiltrate\n",
    "- Consolidation\n",
    "\n",
    "Review the following notes and annotate each as 1 for positive or 0 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL TO SEE QUIZ\n",
    "quiz_pna_annotation1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL TO SEE QUIZ\n",
    "quiz_pna_annotation2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL TO SEE QUIZ\n",
    "quiz_pna_annotation3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL TO SEE QUIZ\n",
    "quiz_pna_annotation4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2\n",
    "The true annotations can be found in `df[\"document_classification\"]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(\"document_classification\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating performace\n",
    "Once you have an annotated dataset, you can compare your NLP's classifications with the annotator's labels.\n",
    "\n",
    "- **True positives** are notes that were annotated as positive by the annotator and also classified as positive by the NLP\n",
    "- **False negatives** were annotated as positives by the annotator but negative by the NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quiz_pna_error_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the NLP on your dataset, you can compare your system's predicted labels ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sumarize the tendency for your model to make false posiitves and false negatives by using two statistics: **precision** and **recall** (also called **positive predictive value** and **sensitivity**).\n",
    "\n",
    "**Precision** is the \n",
    "\n",
    "$$\n",
    "\\text{Precision} = P(\\text{Truth}=1|\\text{Predicted}=1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Recall}  = P(\\text{Predicted}=1|\\text{Truth}=1)\n",
    "$$\n",
    "\n",
    "Intuitively, **precision** answers the question: When my system calls a note positive, how likely is it to be correct? Whereas **sensitivity** answers the question: \"When there is a positive note, how likely is it my system will call it positive\"?\n",
    "\n",
    "Another important metric is the **F1-score**, wich is the harmonic mean of precision and recall:\n",
    "\n",
    "$$\n",
    "\\text{F1} = 2 * \\frac{\\text{Precision}*\\text{Recall}}{\\text{Precision}*\\text{Recall}}\n",
    "$$\n",
    "\n",
    "We can estimate precision and recall by counting up true positives and true negatives:\n",
    "\n",
    "$$\n",
    "\\hat{\\text{P}}\\text{recision} = \\frac{\\text{# TPs}}{\\text{# TPs} + \\text{# FPs}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\text{S}}\\text{ensitivity} = \\frac{\\text{# TPs}}{\\text{# TPs} + \\text{# FNs}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Precision-recall tradeoff\n",
    "There's a trade-off between precision and recall. To illustrate this, consider the following scenario:\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_precision_recall_all_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_precision_recall_all_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we are typically not in the scenario of assigning every note the same prediction, we always have to deal with the trade-off between these two metrics. \n",
    "\n",
    "#### Discussion\n",
    "Which is better - low precision or low recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the performance of the baseline NLP classifier on the training data.\n",
    "\n",
    "#### TODO\n",
    "The cell below counts up the number of true positives, false positives, and false negatives for the baseline system. Use these counts to calculate precision, recall, and F1, then run the quiz to check your answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_true_positives = train[train[\"document_classification\"] == 1][\"baseline_document_classification\"].sum()\n",
    "\n",
    "n_false_positives = train[train[\"document_classification\"] == 0][\"baseline_document_classification\"].sum()\n",
    "\n",
    "n_false_negatives = (1 - train[train[\"document_classification\"] == 1][\"baseline_document_classification\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_true_positives, n_false_negatives, n_false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = ___/(___ + ____)\n",
    "recall = ____\n",
    "f1 = ____*(____*____)/(___+____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_pre_recall_f1.test((precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "How would you interpret these scores? What does the baseline NLP system do well at/not well at?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your NLP system and process texts\n",
    "Now that we have some idea about what our dataset contains, let's starting building an NLP system and reviewing the output. First, build an empty NLP system. Then we'll process the notes in our dataset using our system as is (which doesn't have any rules). Go through the output and review the data. Find some examples of pneumonia that you should extract. Then go through and add rules for each of the following components as needed:\n",
    "\n",
    "1. Add target concept rules to `target_matcher` to identify pneumonia in the text\n",
    "2. Add ConText rules to `context` to improve attribute assertion\n",
    "3. Optionally, add additional rules to `sectionizer` if the section logic is helpful for classifying the entities.\n",
    "4. Build a document classifier which returns `0` or `1` for a doc. A simple version would just use the ConText attributes like `is_negated`, but a more complex version might also use information such as the section of the note.\n",
    "5. Evaluate the system and review errors\n",
    "\n",
    "After adding rules, reprocess your notes and review the output again. Since NLP is a computationally expensive procedure, you might want to work in batches of 10 or so before processing the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = medspacy.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(\"medspacy_sectionizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Concept extraction\n",
    "Add rules to the `target_matcher` component to extract mentions of pneumonia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.target_matcher import TargetRule\n",
    "target_rules = [\n",
    "    \n",
    "]\n",
    "\n",
    "nlp.get_pipe(\"medspacy_target_matcher\").add(target_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ConText\n",
    "Add any modifiers that were not captured with the default rule set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.context import ConTextRule\n",
    "context_rules = [\n",
    "\n",
    "]\n",
    "\n",
    "nlp.get_pipe(\"medspacy_context\").add(context_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Sections\n",
    "Add any section titles which were not detected and led to errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.section_detection import Sectionizer\n",
    "section_rules = [\n",
    "\n",
    "]\n",
    "\n",
    "nlp.get_pipe(\"medspacy_sectionizer\").add(section_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4: Document Classification\n",
    "The function below `classify_pna` takes a doc and returns a `1` if the document is positive for pneumonia and `0` if it is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pna(doc):\n",
    "    for ent in doc.ents:\n",
    "        # Add additional logic as needed\n",
    "        if ent.label_ == \"PNEUMONIA\" and not (ent._.is_negated or ent._.is_historical or ent._.is_hypothetical or ent._.is_family):\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your system is working by running the two cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be negative\n",
    "classify_pna(nlp(\"There is no evidence of pna\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be positive\n",
    "classify_pna(nlp(\"Impression: pneumonia\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "docs = list(nlp.pipe(train[\"text\"]))\n",
    "train[\"doc\"] =  docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5: Evaluate your system on training data\n",
    "After reprocessing your texts and creating `docs` with an updated NLP, run the code below to get performance metrics for your system. The function `evaluate_system` will return a DataFrame with performance characteristics for your system as well as the baseline system.\n",
    "\n",
    "Look at the results and ask the following questions:\n",
    "- What sorts of mistakes does my system appear to be making?\n",
    "- Is precision or recall higher? What does that mean in the context of the research question?\n",
    "- How is it comparing to the baseline NLP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_document_classifications(df, clf):\n",
    "    df[\"my_document_classification\"] = [classify_pna(doc) for doc in df[\"doc\"]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your predictions\n",
    "train = add_document_classifications(train, classify_pna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(train[\"document_classification\"], train[\"my_document_classification\"], labels=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6: Error Analysis\n",
    "Review examples of mistakes your NLP system made. We'll subset the dataframe to look at **false positives** and **false negatives**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = train.query(\"document_classification == 0 & my_document_classification == 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = train.query(\"document_classification == 1 & my_document_classification == 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_fps = MedspaCyVisualizerWidget(list(fps[\"doc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_fns = MedspaCyVisualizerWidget(list(fns[\"doc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final evaluation\n",
    "Once you feel like you're ready, read in the testing data, run your NLP on it, and evaluate it. You should do this **one time** so that it is an honest evaluation of how your system will perform on new, unseen data. Once you see the final results, go through the steps we did above with the training data to understand our performance on the testing set and what sorts of errors happened. How did your final system do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_pneumonia_data(\"test\",)\n",
    "test[\"doc\"] = list(nlp.pipe(test[\"text\"]))\n",
    "test = add_document_classifications(test, classify_pna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test[\"document_classification\"], test[\"my_document_classification\"], labels=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
