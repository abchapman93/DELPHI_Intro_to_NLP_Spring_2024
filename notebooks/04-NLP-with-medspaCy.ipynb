{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65961103",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://github.com/abchapman93/DELPHI_Intro_to_NLP_Spring_2024/blob/main/media/DELPHI-long.png?raw=true\" size=\"20%\">\n",
    "</br>\n",
    "\n",
    "<h1 valign=\"center\" align=\"center\"><font size=\"+150\">Introduction to NLP in Python</br>Spring 2024</font></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c56389f-989e-4aaa-a460-1565e7b68fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://github.com/abchapman93/DELPHI_Intro_to_NLP_Spring_2024/releases/download/v0.1/delphi_nlp_2024-0.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf956da3-d0fb-4ce9-9502-b9cdaf0cb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delphi_nlp_2024 import *\n",
    "from delphi_nlp_2024.quizzes.quizzes import *\n",
    "from delphi_nlp_2024.helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2208d3e3",
   "metadata": {},
   "source": [
    "# NLP with medspaCy\n",
    "This notebook will introduce the Python package `medspaCy`, a toolkit for clinical NLP.\n",
    "\n",
    "# I. Overview\n",
    "As we saw in the last notebook, spaCy doesn't work great for clinical text out of the box. We're interested in extracting different types of information from clinical text than news or Wikipedia articles. Clinical text is also very different from general domain language. \n",
    "- **It is very messy**, with semi-structured formatting from EHR\n",
    "- Clinical documents include **many abbreviations**, some of which are ambiguous\n",
    "- There are **specific tasks** needed in clinical NLP, such as **detecting negation or uncertainty** for concepts in the text\n",
    "\n",
    "One of the most powerful components of spaCy is that is **very customizable**. In addition to working with the default models provided in the core library, you can create your own [custom components](https://spacy.io/usage/processing-pipelines#custom-components) or add your own [extension attributes](https://spacy.io/usage/processing-pipelines#custom-components-attributes). Developers and researchers can then publish their spaCy extensions to the open-source community. Some examples of these openly available libraries are:\n",
    "\n",
    "- [medspacy](https://github.com/medspacy/medspacy): A modular toolkit for various applied clinical NLP tasks\n",
    "- [scispacy](https://allenai.github.io/scispacy/): Includes models trained on biomedical literature\n",
    "- [medCAT](https://github.com/CogStack/MedCAT): Models trained for medical concept extraction\n",
    "\n",
    "## medspacy\n",
    "<img alt=\"MedSpaCy logo\" src=\"https://github.com/medspacy/medspacy/raw/master/images/medspacy_logo.png\">\n",
    "\n",
    "\n",
    "[Medspacy](https://github.com/medspacy/medspacy) is an open-source package maintained by NLP developers at the University of Utah and the US Department of Veterans Affairs. It's built using the popular [spaCy](https://spacy.io/) library and is specifically designed for working with clinical notes. \n",
    "\n",
    "The goal of medSpaCy is to provide flexible, easy-to-use spaCy components for common clinical NLP tasks, such as:\n",
    "\n",
    "- Concept extraction\n",
    "- Negation detection\n",
    "- Document section splitting\n",
    "\n",
    "Here are a couple of papers that used medspaCy:\n",
    "\n",
    "- [Launching into clinical space with medspaCy: a new clinical text processing toolkit in Python\n",
    "](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8861690/)\n",
    "- [A Natural Language Processing System for National\n",
    "COVID-19 Surveillance in the US Department of Veterans Affairs](https://aclanthology.org/2020.nlpcovid19-acl.10.pdf)\n",
    "- [ReHouSED: A novel measurement of Veteran housing stability using natural language processing](https://www.sciencedirect.com/science/article/pii/S153204642100232X?via%3Dihub)\n",
    "- [Assessing mortality prediction through different representation models based on concepts extracted from clinical notes](https://arxiv.org/pdf/2207.10872.pdf)\n",
    "- [A Study into patient similarity through representation learning from medical\n",
    "records ](https://arxiv.org/pdf/2104.14229.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa470e87",
   "metadata": {},
   "source": [
    "## Getting started with medspaCy\n",
    "This notebook will walk show how to use medspaCy to process clinical text and introduce some of the spaCy infrastrcuture. We'll then design some rules to extract concepts from clinical texts.\n",
    "\n",
    "\n",
    "To get started with medspaCy, we'll import the library and then load a **model** which we will call `nlp`. A model in spaCy is the object which processes a note and performs the various steps of text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86395d0-ae79-41d8-a000-aa2616054e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install medspacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import medspacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3b4566",
   "metadata": {},
   "source": [
    "### `nlp`\n",
    "The simplest way to create a model in medspaCy is `medspaCy.load()`. This is a spaCy `English` class. You can also load models for other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a9f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = medspacy.load()\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00712877",
   "metadata": {},
   "source": [
    "### `Doc`\n",
    "To process a text, we call `nlp(text)` and save the result to `doc`. Calling `nlp` on a text returns an object from the `Doc` class. In spaCy, `Doc` objects represent a single text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa679ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Chief complaint: Fever and SOB\"\n",
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f81ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb614b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df22bad5",
   "metadata": {},
   "source": [
    "### `Token`\n",
    "A `Token` is a single word, symbol, or whitespace in a `doc`. When we create a `doc` object, the text broken up into individual tokens. This is called **\"tokenization\"**.\n",
    "\n",
    "**Discussion**: Look at the tokens generated from this text snippet. What can you say about the tokenization method? Is it as simple as splitting up into words every time we reach a whitespace?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a97bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f5944c",
   "metadata": {},
   "source": [
    "If we access a single index of a doc, we get a token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce2401",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = doc[0]\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30593c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_doc_cc_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed548101",
   "metadata": {},
   "source": [
    "### `Span`\n",
    "While a `Token` represents a single word, a `Span` represents one or more words from a `Doc`. We can get a `Span` by slicing a `Doc` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "span = doc[0:3]\n",
    "span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c1b0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "283be07a",
   "metadata": {},
   "source": [
    "## Pipeline Components\n",
    "Under the hood, the `nlp` object goes through a number of sequential steps to process the text. This is called a **pipeline** and it allows us to create modular, independent processing steps when analyzing text. We can see the names of our pipeline components through the `nlp.pipe_names` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca111cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab0c5e",
   "metadata": {},
   "source": [
    "There's also a hidden component which runs before all of them called the `tokenizer`. This splits text up into tokens and creates a `Doc` object, which is then passed on to the rest of the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007105a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842bcbf4",
   "metadata": {},
   "source": [
    "We'll learn more about some of these pipeline components in the following notebooks. First, we'll start with the `target_matcher` component and learn how to extract clinical concepts from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08b41bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, remove some components we don't need\n",
    "nlp.remove_pipe(\"medspacy_pyrush\")\n",
    "nlp.remove_pipe(\"medspacy_context\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5130551b",
   "metadata": {},
   "source": [
    "## Concept Extraction\n",
    "One of the first step in many clinical NLP tasks is identiyfing particular **concepts** in text. These will vary in each use case, but some common examples of concepts are:\n",
    "- Diagnoses\n",
    "- Signs and symptoms\n",
    "- Medications\n",
    "- Tests\n",
    "\n",
    "### TODO\n",
    "For each of the texts below, identify the best description of the concepts **in bold**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1feace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL TO SEE QUIZ\n",
    "quiz_medical_concepts_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f9545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL TO SEE QUIZ\n",
    "quiz_medical_concepts_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09062bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL TO SEE QUIZ\n",
    "quiz_medical_concepts_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c8f51a",
   "metadata": {},
   "source": [
    "The task of extracting these spans of text is called **named entity recognition (NER)**. This can be done using either machine learning models or rule-based models. In this class, we'll focus on building rule-based systems. In rule-based NLP, we define patterns to match concepts in text. SpaCy offers many [rule-based methods](https://spacy.io/usage/rule-based-matching). MedSpaCy uses a pipeline component called `TargetMatcher` and rules defined by a class called `TargetRule`. Extracted concepts will be stored as `Span` objects in `doc.ents`.\n",
    "\n",
    "### `target_matcher`\n",
    "To start adding rules, we'll first need to access the pipeline component. We can do this by calling `nlp.get_pipe(pipe_name)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d92aad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher = nlp.get_pipe(\"medspacy_target_matcher\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7b38b0",
   "metadata": {},
   "source": [
    "Next we need to actually write some rules using the `TargetRule` class. Target rules require two positional arguments:\n",
    "- `literal`: A span of text to match in the text (case insensitive)\n",
    "- `category`: The label to assign to extracted concepts\n",
    "\n",
    "(There are also a few keyword arguments that we'll explore later, but these are the two required arguments.)\n",
    "\n",
    "Let's say that we want to extract patient diagnoses from the following text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47394118",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_text = \"Pt is a 63M w/ h/o metastatic carcinoid tumor, HTN and hyperlipidemia\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b4b00c",
   "metadata": {},
   "source": [
    "There are three diagnoses in this text. The first is `\"metastatic carcinoid tumor\"`. Let's write a rule to capture this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bddb2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.target_matcher import TargetRule\n",
    "rule = TargetRule(\"metastatic carcinoid tumor\", \"DIAGNOSIS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8d7d5b",
   "metadata": {},
   "source": [
    "We can then add it to our target matcher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff5fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher.add(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf25b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher.rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c74daa",
   "metadata": {},
   "source": [
    "Now let's process the text above and see if it's extracted by our NLP model by looking at `doc.ents`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebfe523",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(dx_text)\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951835d6",
   "metadata": {},
   "source": [
    "The `target_matcher` added a `Span` to the doc's entities representing the concept we just extracted. Let's assign this span to the variable `ent`. We can see the concept category by checking the `ent.label_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae08e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = doc.ents[0]\n",
    "print(ent)\n",
    "print(type(ent))\n",
    "print(ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8668a612",
   "metadata": {},
   "source": [
    "`medspaCy` provides some visualization functions which make it easier to look at what has been extracted from the notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a504b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medspacy.visualization import visualize_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9420bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_ent(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c847a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6a3fb24",
   "metadata": {},
   "source": [
    "### TODO\n",
    "Edit the cell below to write a list of rules for extracting the two remaining diagnoses from `dx_text`. Then add them to the target matcher and reprocess the doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6dbf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5871d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher.add(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109e6292",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dx = nlp(dx_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_ent(doc_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fead68",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dx.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef663b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3755f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL TO TEST VALUE\n",
    "test_dx_text.test(doc_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2f25ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4c818a0",
   "metadata": {},
   "source": [
    "### Advanced pattern matching\n",
    "We could pass in simple strings to our `ruler` to extract exact matches. However, there may be lots of small variations in the text we want to extract, and it will grow cumbersome to type out every single possible string. Instead, we'll do some more advanced matching by using **token attribute matching**.\n",
    "\n",
    "SpaCy allows us to write patterns based on not only the exact text, but other linguistic attributes such as **part-of-speech tag**, **numerical properties**, **regular expressions**, and much more. \n",
    "\n",
    "### Example: Chronic Kidney Disease\n",
    "Each of the texts below mention a different stage of Chronic Kidney Disease:\n",
    "\n",
    "---\n",
    "- 76 year old man with CKD Stage 3.\n",
    "- relevant diagnoses: ckd stage 4\n",
    "- The patient has progressed to ckd stage 5\n",
    "---\n",
    "\n",
    "We could write different target rules to match each text, but sometimes there are too many combinations to feasibly write out every option. Instead of trying to think of the near-infinite number of variations, let's write one pattern which will match all of these clinical problems.\n",
    "\n",
    "One way we can do this using **regular expressions**. We can pass a regular expression into the `pattern` argument for a TargetRule. The pattern will be case insensitive.\n",
    "\n",
    "#### TODO\n",
    "Finish the code below to create a rule matching which will match all three examples of CKD using regular expressions. Then add it to the pipeline and test your model. You can test it on the three examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4ec6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"76 year old man with CKD Stage 3.\",\n",
    "    \"relevant diagnoses: ckd stage 4\",\n",
    "    \"The patient has progressed to ckd stage 5\",\n",
    "    \"She was dx'd with CKD in January.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f0de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = TargetRule(\"CKD Stage X\", \"DIAGNOSIS\", \n",
    "                  pattern=___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e13fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matcher = nlp.get_pipe(\"medspacy_target_matcher\")\n",
    "target_matcher.add(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86075d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts:\n",
    "    visualize_ent(nlp(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5118a193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4aab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL TEST VALUE\n",
    "test_ckd_stage_x.test(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111cd8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea1f92fa",
   "metadata": {},
   "source": [
    "## Concept extraction practice\n",
    "Let's return to the example discharge summary we looked at in a previous notebook. Add rules to `target_matcher` that will extract the following concepts from the text:\n",
    "- `\"DIAGNOSIS\"`\n",
    "- `\"MEDICATION\"`\n",
    "- `\"SIGN/SYMPTOM\"`\n",
    "- `\"SOCIAL_DETERMINANT\"`\n",
    "- `\"PROCEDURE\"`\n",
    "\n",
    "It might be useful to work in teams with clinicians or people familiar with these concepts so you can identify and define them. You don't need to extract every concept from the text (there are a lot!) so maybe just go through the note and add a few examples of each conceptt. If you'd like to write more sophisticated rules, it may be helpful to review spaCy's [rule-based NLP documentation](https://spacy.io/usage/rule-based-matching#matcher) (look at the documentation under `Matcher`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c232c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELL TO SEE HINT\n",
    "hint_discharge_summ_target_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4872d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a fresh NLP model\n",
    "nlp = medspacy.load(enable=[\"medspacy_target_matcher\"])\n",
    "target_matcher = nlp.get_pipe(\"medspacy_target_matcher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79e2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [\n",
    "    TargetRule(\"SOB\", \"SIGN/SYMPTOM\"),\n",
    "    TargetRule(\"DOE\", \"SIGN/SYMPTOM\"),\n",
    "    \n",
    "    TargetRule(\"metastatic carcinoid tumor\", \"DIAGNOSIS\"),\n",
    "    TargetRule(\"HTN\", \"DIAGNOSIS\",\n",
    "              pattern=[{\"LOWER\": {\"IN\": [\"htn\", \"hypertension\"]}}]),\n",
    "    TargetRule(\"hyperlipidemia\", \"DIAGNOSIS\"),\n",
    "    \n",
    "    TargetRule(literal=\"congestive heart failure\", category=\"DIAGNOSIS\"),\n",
    "    TargetRule(literal=\" diabetes mellitus type 2\", category=\"DIAGNOSIS\"),\n",
    "    TargetRule(literal=\"basal cell carcinoma\", category=\"DIAGNOSIS\"),\n",
    "    TargetRule(literal=\"atelectasis\", category=\"DIAGNOSIS\"),\n",
    "    TargetRule(literal=\"pneumonia\", category=\"DIAGNOSIS\"),\n",
    "    \n",
    "    TargetRule(literal=\"homeless\", category=\"SOCIAL_DETERMINANT\"),\n",
    "    TargetRule(literal=\"employed\", category=\"SOCIAL_DETERMINANT\"),\n",
    "    TargetRule(literal=\"lives with family\", category=\"SOCIAL_DETERMINANT\",\n",
    "              pattern=[\n",
    "                  {\"LOWER\": \"lives\"},\n",
    "                  {\"LOWER\": \"with\"},\n",
    "                  {\"LIKE_NUM\": True, \"OP\": \"?\"}, \n",
    "                  {\"LOWER\": {\"IN\": [\n",
    "                      \"daughter\",\n",
    "                      \"daughters\",\n",
    "                      \"son\",\n",
    "                      \"sons\",\n",
    "                      \"family\",\n",
    "                  ]}}\n",
    "              ]),\n",
    "    \n",
    "    TargetRule(\"CXR\", \"PROCEDURE\")\n",
    "    \n",
    "]\n",
    "\n",
    "target_matcher.add(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(disch_summ)\n",
    "visualize_ent(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbdb6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
