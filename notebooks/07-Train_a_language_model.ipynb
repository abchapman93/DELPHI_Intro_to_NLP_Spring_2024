{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NOTE: This notebook was adapted from an existing notebook from HuggingFace (original here: https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb) by Kelly Peterson for the 2024 DELPHI NLP course at the University of Utah.  It has been changed to:\n",
        "1. Use a smaller amount of data to allow training a model during a course which is non-optimal.\n",
        "2. Remove one of the tasks in the original notebook\n",
        "3. Show masked language model (MDM) predictions"
      ],
      "metadata": {
        "id": "7PpR5cjL_spr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Also note that you should change the runtime of your notebook to be a TPU or GPU if it is not set already.  Otherwise this could take 10x or 20x longer...  \n",
        "\n",
        "1. Above, select \"Runtime\"\n",
        "2. Then select \"Change Runtime Type\"\n",
        "3. In the dialog that opens, select \"T4 GPU\" or \"TPU\"\n",
        "4. Then click OK"
      ],
      "metadata": {
        "id": "-yxo7LX_BINg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install HuggingFace Transformers and HuggingFace Datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "! pip install datasets transformers transformers[torch] pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBfHx0R9fQxe"
      },
      "source": [
        "Then you need to install Git-LFS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wY1LvqFgfQxe"
      },
      "outputs": [],
      "source": [
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iNXVMORfQxf"
      },
      "source": [
        "Make sure your version of Transformers is at least 4.11.0 so this notebook will work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouHQ3-rIfQxf"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import transformers\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "import torch\n",
        "\n",
        "print(transformers.__version__)\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3KD3WXU3l-O"
      },
      "source": [
        "# Train a language model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAscNNUD3l-P"
      },
      "source": [
        "In this notebook, we'll see how to train a [HuggingFace Transformers](https://github.com/huggingface/transformers) model on a language modeling task. We will cover\n",
        "\n",
        "- Masked language modeling: the model has to predict some tokens that are masked in the input. It still has access to the whole sentence, so it can use the tokens before and after the tokens masked to predict their value.\n",
        "\n",
        "We will see how to easily load and preprocess the dataset for each one of those tasks, and how to use the `Trainer` API to train a model on it.\n",
        "\n",
        "This notebooks assumes you will be using a pretrained tokenizer or you have trained a tokenizer on the corpus you are using, see the [How to train a tokenizer](https://github.com/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb) notebook ([open in colab](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r_n9OWV3l-Q"
      },
      "source": [
        "## Preparing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kswRMhPc3l-Q"
      },
      "source": [
        "For our model, we will use the [Wikitext 2]() dataset as an example. You can load it very easily with the ðŸ¤— Datasets library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2ZRs1cL3l-R"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# This is really big... it would take us 30+ minutes to train, even on GPU...\n",
        "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1-9jepM3l-W"
      },
      "source": [
        "You could theoretically replace the dataset above with any dataset hosted on [the hub](https://huggingface.co/datasets) or use your own files. The example code below shows how you would do this with your own files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxSaGa_l3l-W"
      },
      "outputs": [],
      "source": [
        "# datasets = load_dataset(\"text\", data_files={\"train\": path_to_train.txt, \"validation\": path_to_validation.txt}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY1SwIrY3l-a"
      },
      "source": [
        "You can also load datasets from a csv or a JSON file, see the [full documentation](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3EtYfeHIrIz"
      },
      "source": [
        "To access an actual element, you need to select a split first, then give an index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6HrpprwIrIz"
      },
      "outputs": [],
      "source": [
        "datasets[\"train\"][10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUmphG3IrI3"
      },
      "source": [
        "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur5sNUcZ3l-g"
      },
      "outputs": [],
      "source": [
        "from datasets import ClassLabel\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Uk8NROQ3l-k"
      },
      "outputs": [],
      "source": [
        "show_random_elements(datasets[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(datasets['train']))\n",
        "\n",
        "\n",
        "trimmed_train_dataset = datasets['train'].select(range(1000))\n",
        "trimmed_test_dataset = datasets['test'].select(range(100))\n",
        "\n",
        "print(trimmed_train_dataset.__len__())\n",
        "print(trimmed_test_dataset.__len__())"
      ],
      "metadata": {
        "id": "2AmKymWZpkWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKerdF353l-o"
      },
      "source": [
        "As we can see, some of the texts are a full paragraph of a Wikipedia article while others are just titles or empty lines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-EIELH43l_T"
      },
      "source": [
        "## Masked language modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWk97-Ny3l_T"
      },
      "source": [
        "For masked language modeling (MLM) we are going to preprocess our dataset with one additional step: we will randomly mask some tokens (by replacing them by `[MASK]`) and the labels will be adjusted to only include the masked tokens (we don't have to predict the non-masked tokens). If you use a tokenizer you trained yourself, make sure the `[MASK]` token is among the special tokens you passed during training!\n",
        "\n",
        "We will use the [`bert-base-cased`](https://huggingface.co/bert-based-cased) model architecture for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRTpmyCc3l_T"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"bert-base-cased\"\n",
        "tokenizer_checkpoint = \"sgugger/bert-like-tokenizer\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12F1ulgT3l_V"
      },
      "source": [
        "Load our tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8RCYcvr3l_V"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpOiBrJ13l-y"
      },
      "source": [
        "We can now call the tokenizer on all our texts. This is very simple, using the [`map`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) method from the Datasets library. First we define a function that call the tokenizer on our texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS2m25YM3l-z"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9xVAa3s3l-2"
      },
      "source": [
        "Then we apply it to all the splits in our `datasets` object, using `batched=True` and 4 processes to speed up the preprocessing. We won't need the `text` column afterward, so we discard it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVAO0H8u3l-3"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset = trimmed_train_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
        "tokenized_test_dataset = trimmed_test_dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qik3J_C3l-7"
      },
      "source": [
        "If we now look at an element of our datasets, we will see the text have been replaced by the `input_ids` the model will need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYv_mcKk3l-7"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTuy8UUs3l_X"
      },
      "source": [
        "We group texts together and chunk them in samples of length `block_size`. You can skip that step if your dataset is composed of individual sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obvgcXda3l--"
      },
      "source": [
        "Now for the harder part: we need to concatenate all our texts together then split the result in small chunks of a certain `block_size`. To do this, we will use the `map` method again, with the option `batched=True`. This option actually lets us change the number of examples in the datasets by returning a different number of examples than we got. This way, we can create our new samples from a batch of examples.\n",
        "\n",
        "First, we grab the maximum length our model was pretrained with. This might be a big too big to fit in your GPU RAM, so here we take a bit less at just 128."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVHs5aCA3l-_"
      },
      "outputs": [],
      "source": [
        "# block_size = tokenizer.model_max_length\n",
        "block_size = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpNfGiMw3l_A"
      },
      "source": [
        "Then we write the preprocessing function that will group our texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaAJy5Hu3l_B"
      },
      "outputs": [],
      "source": [
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "        # customize this part to your needs.\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVYPMwEs3l_X"
      },
      "outputs": [],
      "source": [
        "lm_train_dataset = tokenized_train_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    num_proc=4,\n",
        ")\n",
        "\n",
        "lm_test_dataset = tokenized_test_dataset.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    num_proc=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFJ49iHJ3l_Z"
      },
      "source": [
        "Next  we use a model suitable for masked LM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM10A9Za3l_Z"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoModelForMaskedLM\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForMaskedLM.from_config(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Nu8d5hXfQxo"
      },
      "source": [
        "We redefine our `TrainingArguments`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABygxC89fQxo"
      },
      "outputs": [],
      "source": [
        "# we will not push our model to the Internet today... Maybe another day...\n",
        "push_to_hub = False\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \"test-clm\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=push_to_hub,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6uuUnvz3l_b"
      },
      "source": [
        "Finally, we use a special `data_collator`. The `data_collator` is a function that is responsible of taking the samples and batching them in tensors. In the previous example, we had nothing special to do, so we just used the default for this argument. Here we want to do the random-masking. We could do it as a pre-processing step (like the tokenization) but then the tokens would always be masked the same way at each epoch. By doing this step inside the `data_collator`, we ensure this random masking is done in a new way each time we go over the data.\n",
        "\n",
        "To do this masking for us, the library provides a `DataCollatorForLanguageModeling`. We can adjust the probability of the masking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRZ-5v_P3l_b"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqHnWcYC3l_d"
      },
      "source": [
        "Then we just have to pass everything to `Trainer` and begin training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-Y3gNqV3l_d"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_train_dataset,\n",
        "    eval_dataset=lm_test_dataset,\n",
        "    data_collator=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here we go... time to bake this model in the oven... This will take time, even with a short dataset...\n",
        "\n",
        "![Julia Child](https://images.e-flux-systems.com/sc1086_2_07_rosler--julia_child.jpg,400)"
      ],
      "metadata": {
        "id": "bPV1UkkdA3a4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9TFqDG_3l_e"
      },
      "outputs": [],
      "source": [
        "# This will take 1 to 2 minutes when training on GPU..\n",
        "# Much faster than if we used the full dataset which would take 30+ minutes\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDBi0reX3l_g"
      },
      "source": [
        "We can evaluate our model on the validation set. The perplexity is not\n",
        "great since we used a small fraction of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hSaANqj3l_g"
      },
      "outputs": [],
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRhn0RJlfQxp"
      },
      "source": [
        "The perplexity is still quite high since for this demo we trained on a small dataset for a small number of epochs. For a real LM training, you  would need a larger dataset and more epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now for some fun experiments... Let's see how well our model fills in the blanks"
      ],
      "metadata": {
        "id": "wJK-MuVB2yK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_model = transformers.pipeline('fill-mask', model = model, tokenizer = tokenizer, device = torch.cuda.current_device())\n",
        "\n",
        "text = \"The Milky Way is a [MASK] galaxy.\"\n",
        "\n",
        "preds = pred_model(text)\n",
        "\n",
        "#for pred in preds:\n",
        "#    print(f\">>> {pred['sequence']}\")"
      ],
      "metadata": {
        "id": "AgwRqUWryRuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now try your own.  See if the model has any different behavior with context..."
      ],
      "metadata": {
        "id": "Yu85eUmuBuXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO make your own sentences and make sure to include this [MASK] token in your test sentence\n",
        "\n",
        "your_own_text = \"[MASK]\"\n",
        "\n",
        "preds = pred_model(your_own_text)\n",
        "\n",
        "#for pred in preds:\n",
        "#    print(f\">>> {pred['sequence']}\")"
      ],
      "metadata": {
        "id": "c4KdZy2JyXen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G-jaA5kE1KOn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}